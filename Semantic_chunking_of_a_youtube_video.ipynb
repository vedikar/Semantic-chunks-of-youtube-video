{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedikar/Semantic-chunks-of-youtube-video/blob/main/Semantic_chunking_of_a_youtube_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slo5206OZRmB",
        "outputId": "e0f1496f-6f77-4be5-babe-f9397f13a6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.5.27-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.6.2)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.7)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt-dlp-2024.5.27\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8xa4MsubgHn"
      },
      "source": [
        "Youtube video has been downloaded , trying to extract audio from the youtube video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlHfC0zckkK7"
      },
      "source": [
        "Installing ffmpeg to extract audio from the youtube video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkErGPz-bfhQ",
        "outputId": "acc35f95-b169-4c4e-afc5-353843fc34bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVxC2dtHkxxn"
      },
      "source": [
        "To download the youtube video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrvYO6pvebGN",
        "outputId": "2d396465-2019-4cec-9bec-b91480b06498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Sby1uJ_NFIY\n",
            "[youtube] Sby1uJ_NFIY: Downloading webpage\n",
            "[youtube] Sby1uJ_NFIY: Downloading ios player API JSON\n",
            "[youtube] Sby1uJ_NFIY: Downloading player 74204f6c\n",
            "[youtube] Sby1uJ_NFIY: Downloading m3u8 information\n",
            "[info] Sby1uJ_NFIY: Downloading 1 format(s): 248+251\n",
            "[download] Destination: video.mp4.f248.webm\n",
            "\u001b[K[download] 100% of  249.26MiB in \u001b[1;37m00:00:04\u001b[0m at \u001b[0;32m56.77MiB/s\u001b[0m\n",
            "[download] Destination: video.mp4.f251.webm\n",
            "\u001b[K[download] 100% of   28.63MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m53.25MiB/s\u001b[0m\n",
            "[Merger] Merging formats into \"video.mp4.webm\"\n",
            "Deleting original file video.mp4.f251.webm (pass -k to keep)\n",
            "Deleting original file video.mp4.f248.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "# Use yt-dlp to download the video\n",
        "!yt-dlp -o 'video.mp4' https://www.youtube.com/watch?v=Sby1uJ_NFIY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR3y0gBky-6"
      },
      "source": [
        "To list if the video has been downloaded or not!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQJxqMb6fZGI",
        "outputId": "c63cb766-6ba3-468c-edc2-0eeaba25e364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 278M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 13 13:28 sample_data\n",
            "-rw-r--r-- 1 root root 278M Dec 16  2023 video.mp4.webm\n"
          ]
        }
      ],
      "source": [
        "# List the files to ensure the video is downloaded\n",
        "!ls -lh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvPQn8CJk4Qu"
      },
      "source": [
        "Extracting audio from the youtube video using ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFoaQ_7Pjtp3",
        "outputId": "c8d8039e-ca98-44bb-eb5e-d09d1da8b9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, matroska,webm, from 'video.mp4.webm':\n",
            "  Metadata:\n",
            "    ENCODER         : Lavf58.76.100\n",
            "  Duration: 00:26:15.02, start: -0.007000, bitrate: 1480 kb/s\n",
            "  Stream #0:0(eng): Video: vp9 (Profile 0), yuv420p(tv, bt709), 1920x1080, SAR 1:1 DAR 16:9, 30 fps, 30 tbr, 1k tbn, 1k tbc (default)\n",
            "    Metadata:\n",
            "      DURATION        : 00:26:15.000000000\n",
            "  Stream #0:1(eng): Audio: opus, 48000 Hz, stereo, fltp (default)\n",
            "    Metadata:\n",
            "      DURATION        : 00:26:15.021000000\n",
            "Stream mapping:\n",
            "  Stream #0:1 -> #0:0 (opus (native) -> mp3 (libmp3lame))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp3, to 'audio.mp3':\n",
            "  Metadata:\n",
            "    TSSE            : Lavf58.76.100\n",
            "  Stream #0:0(eng): Audio: mp3, 48000 Hz, stereo, fltp (default)\n",
            "    Metadata:\n",
            "      DURATION        : 00:26:15.021000000\n",
            "      encoder         : Lavc58.134.100 libmp3lame\n",
            "size=   41390kB time=00:26:15.00 bitrate= 215.3kbits/s speed=36.3x    \n",
            "video:0kB audio:41390kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000559%\n"
          ]
        }
      ],
      "source": [
        "# Extract audio from the video\n",
        "!ffmpeg -i video.mp4.webm -q:a 0 -map a audio.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smi4WLGzk9LM"
      },
      "source": [
        "List to check if audio file is available or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ilxLBtkUfQ",
        "outputId": "0d3b5996-0450-4dd3-ffc3-845647340c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 319M\n",
            "-rw-r--r-- 1 root root  41M Jun 16 07:53 audio.mp3\n",
            "drwxr-xr-x 1 root root 4.0K Jun 13 13:28 sample_data\n",
            "-rw-r--r-- 1 root root 278M Dec 16  2023 video.mp4.webm\n"
          ]
        }
      ],
      "source": [
        "!ls -lh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-ZecJ9HryS"
      },
      "source": [
        "Code to convert speech to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86AZ-qjxHpfL",
        "outputId": "29e7f96a-f85f-42bb-b04a-b552ff62bfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWIjsD6AH36c",
        "outputId": "731d9c4a-03e9-41bb-cad6-265a91672bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3CvJMToIJtz",
        "outputId": "55511053-33c9-4c07-a915-674b70159a0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='audio.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from pydub import AudioSegment\n",
        "audio = AudioSegment.from_file(\"audio.mp3\")\n",
        "audio.export(\"audio.wav\", format=\"wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScluMfeaFt7o",
        "outputId": "8ee163f7-1f0b-46c9-cbd9-7796452af2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper\n",
            "  Cloning https://github.com/openai/whisper to /tmp/pip-req-build-zrhpmymj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper /tmp/pip-req-build-zrhpmymj\n",
            "  Resolved https://github.com/openai/whisper to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=e2720bf139111218b3191097e79a872f71c945e70ed518a7bc9a8b1835055af0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3uwehkpj/wheels/3d/da/23/a3edaa30ff9d841d474be2db3d17e9230a578e81850b82195d\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/openai/whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZzcfP42jqhM",
        "outputId": "8a1b92f3-d859-4cc9-9190-fbcf0ba0dd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:44<00:00, 34.5MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"medium.en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOQ3aKGByxC",
        "outputId": "e1409dc0-3d47-426a-a178-46578ae1194b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac1jDwkiBuCo",
        "outputId": "99a720c1-d3e3-4720-d422-cf1ef5bc5326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:16<00:00, 93.2MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = whisper.load_model('medium').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o9t1jy1MrG5",
        "outputId": "9df621e8-3403-4bb0-a270-3bf632231914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: 0.0s, End: 2.2s, Text:  Congratulations to you Mr. Raghavan for that.\n",
            "Start: 2.32s, End: 3.66s, Text:  Thank you so much for joining us.\n",
            "Start: 3.74s, End: 4.5600000000000005s, Text:  Over to you.\n",
            "Start: 8.66s, End: 9.36s, Text:  Hi everybody.\n",
            "Start: 9.64s, End: 10.68s, Text:  How are you?\n",
            "Start: 11.92s, End: 13.72s, Text:  Okay, I am not hearing this at all.\n",
            "Start: 13.98s, End: 16.92s, Text:  It's like a post lunch energy downer or something.\n",
            "Start: 18.22s, End: 18.96s, Text:  Let's hear it.\n",
            "Start: 19.74s, End: 20.62s, Text:  Are you guys awake?\n",
            "Start: 21.64s, End: 22.48s, Text:  All right.\n",
            "Start: 23.02s, End: 27.1s, Text:  You better be because we have a superstar guest here.\n",
            "Start: 27.48s, End: 28.98s, Text:  You heard the $41 million\n",
            "Start: 28.98s, End: 31.98s, Text:  and I didn't hear honestly anything she said after that.\n",
            "Start: 33.32s, End: 36.9s, Text:  So we're gonna ask for about $40 million from him\n",
            "Start: 36.9s, End: 38.64s, Text:  by the end of this conversation, okay?\n",
            "Start: 39.6s, End: 40.66s, Text:  But let's get started.\n",
            "Start: 41.88s, End: 43.7s, Text:  I wanna introduce Vivek and Pratyush,\n",
            "Start: 43.72s, End: 45.14s, Text:  his co-founder who's not here.\n",
            "Start: 45.78s, End: 47.82s, Text:  We wanted to start with playing a video\n",
            "Start: 47.82s, End: 50.94s, Text:  of what OpenHati does.\n",
            "Start: 51.14s, End: 53.88s, Text:  I encourage all of you to go to the website,\n",
            "Start: 54.02s, End: 55.64s, Text:  sauram.ai and check it out.\n",
            "Start: 56.74s, End: 58.58s, Text:  But let me start by introducing Vivek.\n",
            "Start: 58.58s, End: 62.4s, Text:  Vivek is a dear friend and he's very, very modest.\n",
            "Start: 62.46s, End: 64.4s, Text:  One of the most modest guys that I know.\n",
            "Start: 64.72s, End: 66.18s, Text:  But his personal journey, Vivek,\n",
            "Start: 66.2s, End: 69.12s, Text:  you've got a PhD from Carnegie Mellon,\n",
            "Start: 69.38s, End: 71.96s, Text:  you started and sold the company to Magma.\n",
            "Start: 72.68s, End: 74.0s, Text:  And Vivek and I moved back to India\n",
            "Start: 74.0s, End: 76.58s, Text:  from we were both in the valley on the same day actually.\n",
            "Start: 77.08s, End: 79.62s, Text:  And you've been in India for the last 16 years.\n",
            "Start: 80.16s, End: 84.74s, Text:  And what most people don't know is your journey at Aadhaar.\n",
            "Start: 85.24s, End: 88.22s, Text:  He spent 13 years selflessly at Aadhaar.\n",
            "Start: 88.58s, End: 90.16s, Text:  Nobody would have heard of him,\n",
            "Start: 90.66s, End: 95.86s, Text:  but he was a pioneering technology visionary behind Aadhaar\n",
            "Start: 95.86s, End: 97.62s, Text:  which we all take for granted today.\n",
            "Start: 99.24s, End: 100.6s, Text:  So please give it out.\n",
            "Start: 102.28s, End: 103.14s, Text:  So honestly, when people,\n",
            "Start: 103.16s, End: 104.36s, Text:  when I think of selfless service,\n",
            "Start: 104.72s, End: 107.56s, Text:  truly selfless service, I always think of Vivek.\n",
            "Start: 107.92s, End: 111.08s, Text:  And since then, he also was at AI for Bharat,\n",
            "Start: 111.28s, End: 112.54s, Text:  which we're gonna touch on,\n",
            "Start: 112.78s, End: 114.74s, Text:  where he met Pratyush's other co-founder.\n",
            "Start: 115.38s, End: 118.2s, Text:  Pratyush had a PhD from ETH at Zurich.\n",
            "Start: 118.2s, End: 120.38s, Text:  He was at IBM research.\n",
            "Start: 120.38s, End: 122.66s, Text:  He was at Microsoft research playing a key role\n",
            "Start: 122.66s, End: 126.28s, Text:  and a faculty at IIT Madras and at AI for Bharat.\n",
            "Start: 126.84s, End: 128.72s, Text:  So that's a little brief introduction about them.\n",
            "Start: 128.78s, End: 130.94s, Text:  These guys are modest, modest engineers.\n",
            "Start: 131.6s, End: 133.3s, Text:  So they don't toot their own horn.\n",
            "Start: 134.2s, End: 137.18s, Text:  So forgive me for tooting their horn in this case.\n",
            "Start: 137.44s, End: 138.96s, Text:  But let's jump right in.\n",
            "Start: 140.6s, End: 143.68s, Text:  About the money, funding, 41 million bucks, man.\n",
            "Start: 143.72s, End: 145.72s, Text:  That's a lot of money, right?\n",
            "Start: 145.74s, End: 147.04s, Text:  Every entrepreneur here is saying,\n",
            "Start: 147.04s, End: 148.56s, Text:  what the hell did these guys do?\n",
            "Start: 149.0s, End: 151.56s, Text:  What did the investors see to write such a big check?\n",
            "Start: 152.64s, End: 155.62s, Text:  No, I think it's a trend,\n",
            "Start: 155.9s, End: 158.06s, Text:  the new trend of what's going on in India.\n",
            "Start: 158.32s, End: 160.1s, Text:  I think that for the very first time,\n",
            "Start: 160.56s, End: 162.86s, Text:  I think the investors have looked at,\n",
            "Start: 163.4s, End: 166.04s, Text:  let's try and build something deep tech out of the country\n",
            "Start: 166.04s, End: 168.94s, Text:  and let's try to figure out how to build something\n",
            "Start: 168.94s, End: 170.88s, Text:  as a foundational technology out of the country.\n",
            "Start: 170.92s, End: 173.2s, Text:  And that's really what's really exciting.\n",
            "Start: 173.2s, End: 176.1s, Text:  And I think that about,\n",
            "Start: 178.5s, End: 181.6s, Text:  as Bala was mentioning for the past 15 years,\n",
            "Start: 181.96s, End: 184.48s, Text:  I've been kind of working in kind of,\n",
            "Start: 185.08s, End: 186.7s, Text:  both digital public infrastructure\n",
            "Start: 187.58s, End: 190.54s, Text:  and kind of a nonprofit kind of things.\n",
            "Start: 190.82s, End: 193.86s, Text:  But when this whole thing of generative AI came about,\n",
            "Start: 195.12s, End: 198.1s, Text:  we said, okay, how can I actually make a difference\n",
            "Start: 198.1s, End: 199.1s, Text:  in this space?\n",
            "Start: 199.12s, End: 201.52s, Text:  And I said, maybe this is the opportunity\n",
            "Start: 201.52s, End: 205.36s, Text:  to actually come out and really build something.\n",
            "Start: 206.38s, End: 208.56s, Text:  And the only way that we realized that you can do it\n",
            "Start: 208.56s, End: 212.66s, Text:  is actually in the private sector.\n",
            "Start: 212.9s, End: 215.16s, Text:  And I think that's, and then we went out there\n",
            "Start: 215.16s, End: 216.58s, Text:  and we said, we want to build something\n",
            "Start: 216.58s, End: 218.18s, Text:  which is a continuation, right?\n",
            "Start: 218.24s, End: 220.58s, Text:  I mean, and fundamentally the question is,\n",
            "Start: 220.88s, End: 222.92s, Text:  the reason of what we want to do at ServoAI\n",
            "Start: 222.92s, End: 227.2s, Text:  is we want to basically make generative AI available\n",
            "Start: 227.2s, End: 229.72s, Text:  and accessible to the people in the country.\n",
            "Start: 229.72s, End: 231.34s, Text:  And that's the intent.\n",
            "Start: 231.62s, End: 233.14s, Text:  And when we said that we want to do this,\n",
            "Start: 233.52s, End: 236.26s, Text:  there was a resonance in the investment community.\n",
            "Start: 236.6s, End: 239.9s, Text:  And I think it's a responsibility to really to show\n",
            "Start: 240.42000000000002s, End: 242.78s, Text:  that something like this can be built out of India.\n",
            "Start: 243.08s, End: 246.58s, Text:  So we see that as confidence and a responsibility.\n",
            "Start: 246.92s, End: 248.7s, Text:  And I also hope it's a trend\n",
            "Start: 248.7s, End: 252.32s, Text:  that there are many more people like us who are backed.\n",
            "Start: 252.52s, End: 253.94s, Text:  Because if you look at it,\n",
            "Start: 254.06s, End: 258.44s, Text:  maybe it's a large number in the Indian context,\n",
            "Start: 258.44s, End: 259.66s, Text:  but in the global context,\n",
            "Start: 260.06s, End: 262.52s, Text:  I think there should be many, many more entrepreneurs\n",
            "Start: 262.52s, End: 264.82s, Text:  who are back to do things in India.\n",
            "Start: 265.68s, End: 267.52s, Text:  Yeah, I'm going to come back to the many more entrepreneurs.\n",
            "Start: 267.92s, End: 270.86s, Text:  I'm obviously going to ask you about Bhavesh's Krutrim.\n",
            "Start: 271.36000000000007s, End: 273.5s, Text:  So we want to come back to that question.\n",
            "Start: 273.96s, End: 276.14s, Text:  But again, $41 million.\n",
            "Start: 276.5s, End: 279.16s, Text:  I mean, all of what you said, you know, $2 million,\n",
            "Start: 279.70000000000005s, End: 281.84s, Text:  that's a good amount of money for a startup\n",
            "Start: 281.84s, End: 284.4s, Text:  which has not yet built anything.\n",
            "Start: 284.92s, End: 286.46s, Text:  What are you going to do with all this money?\n",
            "Start: 288.68s, End: 289.92s, Text:  I can solve the problem.\n",
            "Start: 290.24s, End: 292.0s, Text:  I can have a perfect solution for the problem.\n",
            "Start: 292.3s, End: 293.26s, Text:  I think in the last week,\n",
            "Start: 293.38s, End: 295.38s, Text:  I've got lots of calls from lots of people\n",
            "Start: 296.08000000000004s, End: 297.7s, Text:  telling me how I can do.\n",
            "Start: 297.7s, End: 298.46s, Text:  No, but...\n",
            "Start: 298.46s, End: 299.34s, Text:  I know you first, okay?\n",
            "Start: 299.34s, End: 300.88s, Text:  I'll be landed in the country the same day.\n",
            "Start: 300.98s, End: 301.92s, Text:  I'm in the front of the queue.\n",
            "Start: 303.40000000000003s, End: 307.08s, Text:  No, but honestly, I think the key thing in this\n",
            "Start: 307.08s, End: 309.44s, Text:  is to putting together an amazing team.\n",
            "Start: 309.6s, End: 311.3s, Text:  And we actually have an amazing team,\n",
            "Start: 311.3s, End: 313.64s, Text:  but we believe that it is talent\n",
            "Start: 313.64s, End: 315.12s, Text:  that will drive this kind of thing.\n",
            "Start: 315.22s, End: 318.04s, Text:  And so it is to get key talent.\n",
            "Start: 318.04s, End: 319.66s, Text:  And of course, the other thing is compute.\n",
            "Start: 319.74s, End: 323.28s, Text:  This is extremely expensive, compute-wise,\n",
            "Start: 323.58s, End: 325.24s, Text:  to actually do these kinds of things.\n",
            "Start: 325.42s, End: 327.96s, Text:  And I think that those are the two primary things\n",
            "Start: 328.79999999999995s, End: 330.38s, Text:  that we'd use this for.\n",
            "Start: 331.02s, End: 331.44s, Text:  Okay.\n",
            "Start: 332.84s, End: 334.8s, Text:  I'm computing in my own head as an entrepreneur.\n",
            "Start: 334.94s, End: 337.26s, Text:  Talent, okay, you have like 20, 15 people.\n",
            "Start: 337.6s, End: 338.84s, Text:  How much are you paying these guys?\n",
            "Start: 339.0s, End: 340.72s, Text:  But okay, we won't touch on that.\n",
            "Start: 341.54s, End: 343.64s, Text:  But let's talk about what you guys actually built.\n",
            "Start: 343.82s, End: 344.96s, Text:  What is OpenHati?\n",
            "Start: 345.16s, End: 347.4s, Text:  How would you explain OpenHati to many people here\n",
            "Start: 347.4s, End: 348.9s, Text:  who might not have known about it?\n",
            "Start: 349.54s, End: 352.62s, Text:  So I think OpenHati is, so first of all, right,\n",
            "Start: 352.7s, End: 356.36s, Text:  we come from, I personally come from the open source\n",
            "Start: 356.36s, End: 359.96s, Text:  ecosystem and also from the DPI ecosystem.\n",
            "Start: 360.38s, End: 362.46s, Text:  So we believe that for this to work,\n",
            "Start: 362.66s, End: 365.24s, Text:  we need the ecosystem to be successful.\n",
            "Start: 365.26s, End: 368.48s, Text:  And as a result of that, one of the first things we did was,\n",
            "Start: 368.92s, End: 370.78s, Text:  hey, there are these open source\n",
            "Start: 370.78s, End: 372.72s, Text:  large language models that exist, right?\n",
            "Start: 372.74s, End: 376.18s, Text:  I mean, everybody knows about the llama family from Meta.\n",
            "Start: 376.18s, End: 378.5s, Text:  There are others like Mistral.\n",
            "Start: 378.66s, End: 383.12s, Text:  There are a bunch of open source large language models.\n",
            "Start: 383.62s, End: 385.92s, Text:  And then we said, is there any way\n",
            "Start: 385.92s, End: 388.86s, Text:  that they can existing open source model\n",
            "Start: 388.86s, End: 390.84s, Text:  and teach it language skills, right?\n",
            "Start: 390.88s, End: 395.16s, Text:  I mean, and that is really what we said\n",
            "Start: 395.16s, End: 397.18s, Text:  that can we do something like that?\n",
            "Start: 397.44s, End: 400.22s, Text:  And is this a relatively frugal way\n",
            "Start: 400.22s, End: 406.16s, Text:  of actually making models work in a way\n",
            "Start: 406.18s, End: 407.44s, Text:  in diverse languages?\n",
            "Start: 408.0s, End: 409.32s, Text:  Because the truth is still today,\n",
            "Start: 409.5s, End: 412.9s, Text:  I mean, if you look at the amount of data and knowledge,\n",
            "Start: 413.16s, End: 414.94s, Text:  it is still English dominates these things.\n",
            "Start: 415.3s, End: 417.56s, Text:  And I think that how do you actually take\n",
            "Start: 417.56s, End: 419.78s, Text:  and make it understand Indian language,\n",
            "Start: 420.06s, End: 422.52s, Text:  understand Indian context, and all of those things\n",
            "Start: 422.52s, End: 425.04s, Text:  in actually in an efficient way?\n",
            "Start: 425.1s, End: 427.14s, Text:  And therefore, this was an attempt to do that.\n",
            "Start: 428.08s, End: 432.56s, Text:  And it's a OpenHati is currently based on the llama\n",
            "Start: 432.56s, End: 434.54s, Text:  seven billion model, but we'll be releasing\n",
            "Start: 434.54s, End: 436.66s, Text:  many more models in different languages,\n",
            "Start: 436.94s, End: 439.14s, Text:  different sizes, and things like that\n",
            "Start: 439.14s, End: 441.88s, Text:  as part of this series.\n",
            "Start: 442.56s, End: 444.92s, Text:  And of course, we will be building further models\n",
            "Start: 444.92s, End: 447.86s, Text:  on those and doing other things to actually,\n",
            "Start: 447.96s, End: 450.0s, Text:  and we'll also have endpoints that people can use.\n",
            "Start: 450.18s, End: 452.6s, Text:  So therefore, it's definitely something\n",
            "Start: 452.6s, End: 455.34s, Text:  that people can use to things.\n",
            "Start: 456.14s, End: 460.12s, Text:  And that's the essence of what this OpenHati is.\n",
            "Start: 460.2s, End: 461.84s, Text:  So what does it mean to people in the audience here\n",
            "Start: 461.84s, End: 463.66s, Text:  who are either doing their own startups\n",
            "Start: 463.66s, End: 466.84s, Text:  or a business or developers,\n",
            "Start: 467.52s, End: 469.26s, Text:  how should they look at OpenAI?\n",
            "Start: 469.82s, End: 470.5s, Text:  Sorry, Sarvam.\n",
            "Start: 470.98s, End: 476.02s, Text:  No, no, I think the way you look at it is that\n",
            "Start: 476.02s, End: 478.38s, Text:  one of the important things that we are doing\n",
            "Start: 478.38s, End: 480.56s, Text:  is we're not just building models.\n",
            "Start: 481.0s, End: 483.94s, Text:  We are also going to be building a platform,\n",
            "Start: 484.24s, End: 488.06s, Text:  a platform for developers where you can actually use\n",
            "Start: 488.06s, End: 490.5s, Text:  a combination of various different kinds of models,\n",
            "Start: 490.5s, End: 493.1s, Text:  some which are from us, some which are open source,\n",
            "Start: 493.12s, End: 494.4s, Text:  some which may not be open source,\n",
            "Start: 494.62s, End: 497.26s, Text:  and actually to actually pull together and figure out\n",
            "Start: 497.26s, End: 502.9s, Text:  how to deploy generative AI applications at scale\n",
            "Start: 502.9s, End: 505.4s, Text:  and understand and evaluate their performance\n",
            "Start: 505.4s, End: 506.36s, Text:  in an efficient manner.\n",
            "Start: 506.6s, End: 508.72s, Text:  And that's something that we are planning to do.\n",
            "Start: 509.18s, End: 512.72s, Text:  And this platform is, in the next couple of months,\n",
            "Start: 512.82s, End: 513.8s, Text:  will be coming out there.\n",
            "Start: 513.96s, End: 515.28s, Text:  It will be available to developers.\n",
            "Start: 515.68s, End: 517.14s, Text:  But of course, those who want to start\n",
            "Start: 517.14s, End: 520.0s, Text:  with the open source things and hack with that,\n",
            "Start: 520.0s, End: 522.0s, Text:  of course, please go ahead and do that as well.\n",
            "Start: 522.6s, End: 523.46s, Text:  That's phenomenal.\n",
            "Start: 525.18s, End: 528.34s, Text:  But how does it compare to OpenAI itself or Google?\n",
            "Start: 530.3s, End: 532.98s, Text:  See, at least the things that we are doing now.\n",
            "Start: 533.2s, End: 536.06s, Text:  One of the things that, when we thought about\n",
            "Start: 536.06s, End: 538.54s, Text:  building Sarvam, we said we want to build\n",
            "Start: 539.1999999999999s, End: 541.32s, Text:  a full-stack generative AI company,\n",
            "Start: 541.96s, End: 544.06s, Text:  and different people have, and our understanding\n",
            "Start: 544.06s, End: 545.94s, Text:  of full-stack is that we need to know\n",
            "Start: 545.94s, End: 548.18s, Text:  how to train models from scratch.\n",
            "Start: 548.18s, End: 552.28s, Text:  We need to know how to figure out how to deploy models\n",
            "Start: 552.28s, End: 553.92s, Text:  to solve real-world use cases.\n",
            "Start: 554.34s, End: 557.26s, Text:  And we need to play in the ecosystem to make sure\n",
            "Start: 557.26s, End: 562.28s, Text:  that we can actually deploy population scale applications.\n",
            "Start: 562.54s, End: 564.88s, Text:  So we were thinking about all of these things.\n",
            "Start: 565.24s, End: 567.16s, Text:  But still, the models we were talking about\n",
            "Start: 567.16s, End: 569.36s, Text:  are fairly small models.\n",
            "Start: 569.42s, End: 570.72s, Text:  They are fairly small models, right?\n",
            "Start: 570.86s, End: 574.04s, Text:  The seven to maybe up to 70 billion kind of range\n",
            "Start: 574.04s, End: 574.94s, Text:  we're talking about.\n",
            "Start: 574.94s, End: 578.08s, Text:  While these models like OpenAI and Google\n",
            "Start: 578.08s, End: 580.14s, Text:  are obviously much bigger models, right?\n",
            "Start: 580.42s, End: 583.56s, Text:  But we want to understand the techniques\n",
            "Start: 583.56s, End: 585.32s, Text:  and be able to build that muscle\n",
            "Start: 585.32s, End: 589.9s, Text:  to do all of these things, to make it available to people.\n",
            "Start: 590.06s, End: 592.58s, Text:  Now those models are, I mean, as I said,\n",
            "Start: 592.66s, End: 596.22s, Text:  I think that there is space for all of those things.\n",
            "Start: 596.48s, End: 600.1s, Text:  And I think as even Sridhar was talking about earlier\n",
            "Start: 600.1s, End: 603.12s, Text:  in the day, we believe that these smaller models\n",
            "Start: 603.12s, End: 608.78s, Text:  can do many, many kind of domain specific tasks\n",
            "Start: 608.78s, End: 612.36s, Text:  extremely well, probably even better than the larger models.\n",
            "Start: 612.7s, End: 614.62s, Text:  And that is really one of the key areas.\n",
            "Start: 615.14s, End: 617.86s, Text:  And so therefore the value of these kinds of things, right?\n",
            "Start: 618.18s, End: 620.38s, Text:  We are not aiming in these more set of models\n",
            "Start: 620.38s, End: 622.28s, Text:  to build any AGI, right?\n",
            "Start: 622.28s, End: 623.5s, Text:  That's not our goal here.\n",
            "Start: 623.72s, End: 626.3s, Text:  Our goal is to make things that work extremely well\n",
            "Start: 626.3s, End: 628.8s, Text:  for domain specific use cases\n",
            "Start: 628.8s, End: 631.64s, Text:  or increase accessibility through language\n",
            "Start: 631.64s, End: 633.22s, Text:  and all of those kinds of things.\n",
            "Start: 633.4s, End: 634.96s, Text:  And obviously all of this unique to India.\n",
            "Start: 635.18s, End: 636.5s, Text:  But what is unique about India?\n",
            "Start: 636.88s, End: 639.32s, Text:  I mean, like what is, is there anything special\n",
            "Start: 639.32s, End: 643.3s, Text:  in our ecosystem that makes small models\n",
            "Start: 643.3s, End: 645.98s, Text:  focused with Indian languages better for,\n",
            "Start: 646.02s, End: 646.98s, Text:  more suited for our problems?\n",
            "Start: 647.94s, End: 651.68s, Text:  So I think that, I mean, there are quite a few things\n",
            "Start: 651.68s, End: 653.3s, Text:  that are unique about India, right?\n",
            "Start: 653.74s, End: 657.72s, Text:  The first thing is I think that we are a voice first nation.\n",
            "Start: 657.72s, End: 660.56s, Text:  So therefore I think voice has to be the core\n",
            "Start: 660.56s, End: 661.52s, Text:  to doing things.\n",
            "Start: 662.52s, End: 665.84s, Text:  The other thing, of course, India is extremely,\n",
            "Start: 666.12s, End: 669.62s, Text:  it's a cost conscious country from a cost perspective.\n",
            "Start: 669.9s, End: 672.5s, Text:  Now, I would say that there are lots of interesting\n",
            "Start: 672.5s, End: 675.28s, Text:  use cases where you can use open AI\n",
            "Start: 675.28s, End: 677.24s, Text:  and the cost structure works that when,\n",
            "Start: 677.64s, End: 678.7s, Text:  depending on your application.\n",
            "Start: 678.96s, End: 681.52s, Text:  But when you want to scale things to a massive level\n",
            "Start: 681.52s, End: 684.02s, Text:  and make it work, then you have to figure out\n",
            "Start: 684.02s, End: 685.12s, Text:  how small models work.\n",
            "Start: 685.12s, End: 688.34s, Text:  So that's something that is also specific to India.\n",
            "Start: 688.64s, End: 690.12s, Text:  The third thing which is specific to India\n",
            "Start: 690.12s, End: 692.98s, Text:  is really the success that India has had\n",
            "Start: 692.98s, End: 695.72s, Text:  in building all this digital public infrastructure.\n",
            "Start: 696.04s, End: 698.44s, Text:  When you add the AI layer on top of it,\n",
            "Start: 698.74s, End: 702.2s, Text:  then you can actually get dramatic, you know,\n",
            "Start: 702.4s, End: 706.14s, Text:  dramatic, I think, multiplicative combinatorial effects\n",
            "Start: 706.14s, End: 707.6s, Text:  based on doing things like that.\n",
            "Start: 707.88s, End: 708.84s, Text:  That's a phenomenal point.\n",
            "Start: 709.02s, End: 710.88s, Text:  Like, you know, it's like DPI to the power of AI\n",
            "Start: 710.88s, End: 711.84s, Text:  almost in some ways.\n",
            "Start: 712.2s, End: 714.34s, Text:  And as a part of building other,\n",
            "Start: 714.34s, End: 716.26s, Text:  no better person than you.\n",
            "Start: 717.7s, End: 720.34s, Text:  So in summary, what I'm hearing is small models\n",
            "Start: 720.34s, End: 723.84s, Text:  specialize with, trained with Indic specific language data\n",
            "Start: 723.84s, End: 727.6s, Text:  suited for Indian problems at a compelling cost point\n",
            "Start: 727.6s, End: 728.9s, Text:  will be suited for us.\n",
            "Start: 728.94s, End: 731.42s, Text:  We're not solving some word autonomous vehicles\n",
            "Start: 731.42s, End: 732.72s, Text:  or some complex problem.\n",
            "Start: 732.76s, End: 734.6s, Text:  We're solving some basic problems\n",
            "Start: 734.6s, End: 737.64s, Text:  specifically focused on voice with multiple languages.\n",
            "Start: 738.06s, End: 739.38s, Text:  That is what you see as the future.\n",
            "Start: 739.46s, End: 741.0s, Text:  Am I paraphrasing this correctly?\n",
            "Start: 741.0s, End: 741.8s, Text:  No, yeah.\n",
            "Start: 741.9s, End: 743.9s, Text:  So I think that certainly, I mean,\n",
            "Start: 744.06s, End: 746.72s, Text:  voice and Indian languages are an important part\n",
            "Start: 746.72s, End: 749.48s, Text:  of our strategy, but we will be building, you know,\n",
            "Start: 749.76s, End: 752.34s, Text:  custom models to solve various other kinds of problems\n",
            "Start: 752.34s, End: 753.2s, Text:  as well, right?\n",
            "Start: 753.2s, End: 757.12s, Text:  It's not just limited to, I think, in different domains,\n",
            "Start: 757.4s, End: 758.38s, Text:  working in different domains,\n",
            "Start: 758.92s, End: 761.48s, Text:  making building things based on unique data\n",
            "Start: 761.48s, End: 763.46s, Text:  that enterprises have and things like that.\n",
            "Start: 763.46s, End: 765.1s, Text:  So that's something that we'll also look at.\n",
            "Start: 765.38s, End: 765.8s, Text:  Fair enough.\n",
            "Start: 766.26s, End: 768.88s, Text:  So coming back to the elephant in the room,\n",
            "Start: 768.88s, End: 771.1s, Text:  no pun intended with open Hathi.\n",
            "Start: 771.7s, End: 774.06s, Text:  What about Babi Shakerwal and Kruthram?\n",
            "Start: 774.54s, End: 775.3s, Text:  What is your take on that?\n",
            "Start: 775.38s, End: 776.34s, Text:  No, I think it's great.\n",
            "Start: 776.6s, End: 778.2s, Text:  I think it's wonderful, right?\n",
            "Start: 778.24s, End: 782.58s, Text:  I mean, the fact that the technology AI is so important\n",
            "Start: 782.58s, End: 785.56s, Text:  that we need multiple people working on it.\n",
            "Start: 785.58s, End: 787.76s, Text:  The fact that there are other people thinking\n",
            "Start: 787.76s, End: 790.94s, Text:  is actually validates that this is an important problem\n",
            "Start: 790.94s, End: 791.68s, Text:  to be solved.\n",
            "Start: 792.16s, End: 796.76s, Text:  And I think that, and we need everybody to come together\n",
            "Start: 796.76s, End: 797.44s, Text:  and do that.\n",
            "Start: 797.44s, End: 798.98s, Text:  So I really welcome that.\n",
            "Start: 799.1s, End: 800.06s, Text:  I think it's great.\n",
            "Start: 800.48s, End: 802.88s, Text:  And I think that there'll be different people\n",
            "Start: 802.88s, End: 804.0s, Text:  will have different takes\n",
            "Start: 804.0s, End: 806.18s, Text:  as to how to solve this kind of problem.\n",
            "Start: 806.58s, End: 808.52s, Text:  And hopefully as a result of that,\n",
            "Start: 808.88s, End: 810.48s, Text:  the entire ecosystem benefits.\n",
            "Start: 811.66s, End: 812.68s, Text:  I have one more question.\n",
            "Start: 812.72s, End: 814.9s, Text:  And then I wanna talk about some of the predictions\n",
            "Start: 814.9s, End: 816.2s, Text:  that you've boldly made.\n",
            "Start: 816.38s, End: 818.26s, Text:  So Vivek, I usually ask people about,\n",
            "Start: 818.36s, End: 819.24s, Text:  what do you think the future will be?\n",
            "Start: 819.28s, End: 820.56s, Text:  And everybody usually hedges.\n",
            "Start: 821.12s, End: 822.74s, Text:  I asked Vivek, what do you think is gonna happen\n",
            "Start: 823.2s, End: 824.64s, Text:  by December, 2024?\n",
            "Start: 824.9s, End: 826.12s, Text:  What do you think sitting in this room,\n",
            "Start: 826.12s, End: 827.94s, Text:  one year later, we can expect?\n",
            "Start: 828.32s, End: 829.82s, Text:  And he made three bold predictions.\n",
            "Start: 830.3s, End: 831.6s, Text:  So I wanna talk about that.\n",
            "Start: 831.64s, End: 832.98s, Text:  Before that, I have one last question.\n",
            "Start: 833.8s, End: 835.28s, Text:  What are the top three applications\n",
            "Start: 835.28s, End: 837.26s, Text:  that you think are relevant for India?\n",
            "Start: 838.34s, End: 839.52s, Text:  You heard Sridhar talk about medical.\n",
            "Start: 840.02s, End: 841.26s, Text:  What, no, any quick summary.\n",
            "Start: 841.38s, End: 844.44s, Text:  What do you think the top three apps are for India, for AI?\n",
            "Start: 845.04s, End: 847.4s, Text:  So I mean, I think that, as you said,\n",
            "Start: 847.48s, End: 851.6s, Text:  things like education and medical are clearly areas\n",
            "Start: 852.56s, End: 855.04s, Text:  where I think that things can be leveraged.\n",
            "Start: 855.04s, End: 858.14s, Text:  The whole idea of all these kind of,\n",
            "Start: 858.14s, End: 861.08s, Text:  the DPI aspect of it is another major application\n",
            "Start: 861.08s, End: 862.16s, Text:  where things can happen.\n",
            "Start: 862.38s, End: 864.7s, Text:  And here I'm talking about country-specific work.\n",
            "Start: 864.9s, End: 867.28s, Text:  And I think the whole idea, which Sridhar also talked about,\n",
            "Start: 867.34s, End: 870.0s, Text:  was the concept of software, right?\n",
            "Start: 870.0s, End: 872.24s, Text:  And I think that, and clearly we have\n",
            "Start: 872.24s, End: 873.82s, Text:  a very large software industry,\n",
            "Start: 874.26s, End: 876.98s, Text:  and how to reimagine those things in this context\n",
            "Start: 876.98s, End: 878.3s, Text:  is also something that's gonna be.\n",
            "Start: 879.3000000000001s, End: 880.08s, Text:  Fair enough.\n",
            "Start: 881.02s, End: 883.64s, Text:  Are you guys ready for Vivek Raghavan's bold predictions?\n",
            "Start: 884.36s, End: 884.72s, Text:  Yes?\n",
            "Start: 885.3s, End: 886.76s, Text:  No, I'm not hearing any yeses.\n",
            "Start: 886.82s, End: 887.74s, Text:  This is like a big deal.\n",
            "Start: 888.06s, End: 889.84s, Text:  He's like one of the smartest guys that I know.\n",
            "Start: 890.14s, End: 891.26s, Text:  He wants to make three predictions.\n",
            "Start: 891.3s, End: 892.18s, Text:  You don't want to hear it?\n",
            "Start: 893.4s, End: 893.98s, Text:  All right.\n",
            "Start: 894.2s, End: 897.8s, Text:  So I asked him, what do you think, a year later,\n",
            "Start: 897.94s, End: 899.14s, Text:  what do you think we can expect?\n",
            "Start: 899.64s, End: 900.9s, Text:  And he came up with three things,\n",
            "Start: 900.92s, End: 902.9s, Text:  and usually people give very blah answers\n",
            "Start: 902.9s, End: 904.42s, Text:  when you ask a question like this,\n",
            "Start: 904.46s, End: 905.72s, Text:  because they don't want to be caught wrong.\n",
            "Start: 906.16s, End: 907.62s, Text:  Not Vivek, Vivek is bold.\n",
            "Start: 908.4s, End: 910.04s, Text:  So he basically said three things,\n",
            "Start: 910.06s, End: 911.46s, Text:  and I'm gonna list out the three things\n",
            "Start: 911.46s, End: 913.22s, Text:  and then ask him about it.\n",
            "Start: 913.22s, End: 914.62s, Text:  So number one, he says,\n",
            "Start: 915.26s, End: 917.84s, Text:  I would prefer to talk to an automated customer service\n",
            "Start: 917.84s, End: 919.28s, Text:  than a real person,\n",
            "Start: 919.74s, End: 921.38s, Text:  because they'll give me a better answer.\n",
            "Start: 921.6s, End: 923.88s, Text:  So that is Vivek Raghavan's prediction number one.\n",
            "Start: 925.36s, End: 927.68s, Text:  So number two is that when everybody is talking\n",
            "Start: 927.68s, End: 929.06s, Text:  about a GPU shortage,\n",
            "Start: 930.02s, End: 932.58s, Text:  Vivek predicts that there'll be a GPU glut in India.\n",
            "Start: 932.68s, End: 933.9s, Text:  He thinks there'll be too much GPU.\n",
            "Start: 934.82s, End: 937.8s, Text:  So if you want a short NVIDIA stock, this is a good time.\n",
            "Start: 939.08s, End: 941.3s, Text:  And number three, which was extremely unexpected,\n",
            "Start: 941.9799999999999s, End: 944.3s, Text:  he said some companies will suddenly die.\n",
            "Start: 945.46s, End: 947.76s, Text:  Okay, so Vivek, these are not what I expected.\n",
            "Start: 949.72s, End: 952.88s, Text:  So do you want to quickly talk about each of them,\n",
            "Start: 953.3s, End: 954.54s, Text:  why you just came up with these,\n",
            "Start: 954.62s, End: 956.62s, Text:  and then we'll throw the open for audience questions.\n",
            "Start: 956.78s, End: 959.2199999999999s, Text:  So I don't think I quite said it the way that\n",
            "Start: 960.02s, End: 960.34s, Text:  Pallavi is panicking.\n",
            "Start: 960.34s, End: 961.48s, Text:  Sorry, I'm the marketing guy here.\n",
            "Start: 962.4399999999999s, End: 963.74s, Text:  But it's interesting.\n",
            "Start: 964.04s, End: 967.6s, Text:  But I think that the first thing that we said is,\n",
            "Start: 967.76s, End: 970.58s, Text:  I think that, and I don't think that this is,\n",
            "Start: 970.58s, End: 973.38s, Text:  I think there will come a time when,\n",
            "Start: 975.0600000000001s, End: 977.6s, Text:  in areas of customer service, et cetera,\n",
            "Start: 977.66s, End: 980.42s, Text:  when you want to do something very specific.\n",
            "Start: 980.86s, End: 983.82s, Text:  Today, when you call some kind of a bot,\n",
            "Start: 984.16s, End: 986.86s, Text:  you actually end up, you mostly try to disconnect the call,\n",
            "Start: 987.14s, End: 990.94s, Text:  or you're extremely upset that you're talking to a bot.\n",
            "Start: 991.26s, End: 993.02s, Text:  But I think that there will come a time,\n",
            "Start: 993.06s, End: 995.54s, Text:  and I'm predicting it is sooner than later,\n",
            "Start: 995.86s, End: 999.46s, Text:  that you will actually get better responses from the bot\n",
            "Start: 999.46s, End: 1001.68s, Text:  than what the human representative,\n",
            "Start: 1001.96s, End: 1003.74s, Text:  at least the average human representative\n",
            "Start: 1003.74s, End: 1005.64s, Text:  that you could talk to, could give.\n",
            "Start: 1006.06s, End: 1007.26s, Text:  And I think that that's just,\n",
            "Start: 1007.66s, End: 1010.66s, Text:  I just said that there will come a time where you know\n",
            "Start: 1010.66s, End: 1012.1s, Text:  it's not a human you're talking to,\n",
            "Start: 1012.36s, End: 1015.76s, Text:  but it's probably more likely to solve your intent\n",
            "Start: 1016.66s, End: 1017.82s, Text:  than the human person.\n",
            "Start: 1017.94s, End: 1023.08s, Text:  That's just something that I think could happen.\n",
            "Start: 1023.36s, End: 1026.26s, Text:  Okay, definitely controversial, but we'll let it go.\n",
            "Start: 1026.34s, End: 1027.66s, Text:  What about the GPU glut?\n",
            "Start: 1028.3400000000001s, End: 1030.22s, Text:  No, no, yeah, so I don't think that,\n",
            "Start: 1030.26s, End: 1032.66s, Text:  so I think that the fact that there is\n",
            "Start: 1032.66s, End: 1034.98s, Text:  a tremendous shortage right now,\n",
            "Start: 1035.2s, End: 1036.68s, Text:  I think that shortage will ease,\n",
            "Start: 1036.9s, End: 1039.82s, Text:  because that is how the cycles of things go, right?\n",
            "Start: 1040.32s, End: 1042.36s, Text:  When, you know, I think the fact that\n",
            "Start: 1042.36s, End: 1044.9s, Text:  there was such a severe shortage last year,\n",
            "Start: 1044.9s, End: 1048.46s, Text:  you know, basically caused a number of different players\n",
            "Start: 1048.46s, End: 1051.04s, Text:  to ramp up in various kinds of forms.\n",
            "Start: 1051.32s, End: 1054.36s, Text:  And I think that that will always go in a cycle.\n",
            "Start: 1054.6s, End: 1057.08s, Text:  But you may, we may find out that there are many,\n",
            "Start: 1057.08s, End: 1058.62s, Text:  many more interesting problems\n",
            "Start: 1058.62s, End: 1059.94s, Text:  that people will be able to solve.\n",
            "Start: 1060.9199999999998s, End: 1064.64s, Text:  I still remember, you know,\n",
            "Start: 1065.0s, End: 1068.86s, Text:  we were at a Gen. AI event in Bangalore,\n",
            "Start: 1068.96s, End: 1071.06s, Text:  and we were talking to people, and we said,\n",
            "Start: 1071.06s, End: 1073.5s, Text:  you know, how many people have access to,\n",
            "Start: 1073.5s, End: 1075.3s, Text:  you know, four A-hundreds?\n",
            "Start: 1075.3s, End: 1076.54s, Text:  This was the question that I'd asked,\n",
            "Start: 1076.8s, End: 1077.82s, Text:  and nobody in the room,\n",
            "Start: 1077.82s, End: 1080.24s, Text:  and these are all extremely enthusiastic Gen. AI people,\n",
            "Start: 1080.54s, End: 1081.44s, Text:  and nobody had access.\n",
            "Start: 1081.7s, End: 1083.4s, Text:  And I think that thing is going to change.\n",
            "Start: 1083.44s, End: 1085.48s, Text:  You will be able to get these kinds of things,\n",
            "Start: 1085.48s, End: 1087.5s, Text:  and people who want to hack and do things\n",
            "Start: 1087.5s, End: 1089.18s, Text:  will have access to these things\n",
            "Start: 1089.18s, End: 1093.88s, Text:  at, without, you know, having to write a, you know,\n",
            "Start: 1094.12s, End: 1094.82s, Text:  a major check.\n",
            "Start: 1095.02s, End: 1096.68s, Text:  So Vivek is also a semiconductor guy\n",
            "Start: 1096.68s, End: 1097.8s, Text:  before he went into Aadhaar,\n",
            "Start: 1097.94s, End: 1100.3s, Text:  so I would take his predictions very seriously.\n",
            "Start: 1100.66s, End: 1102.64s, Text:  So I don't know what I, I'm going to sell my in-media stock.\n",
            "Start: 1103.22s, End: 1104.58s, Text:  No, no, no, I would not do that,\n",
            "Start: 1104.6s, End: 1106.08s, Text:  but that's not what I said.\n",
            "Start: 1107.32s, End: 1109.64s, Text:  I want to blame you for this, if it goes up.\n",
            "Start: 1111.08s, End: 1112.48s, Text:  But the third one is pretty strange.\n",
            "Start: 1113.08s, End: 1114.96s, Text:  You know, companies are born, companies die,\n",
            "Start: 1114.96s, End: 1117.72s, Text:  but you said some companies will suddenly die.\n",
            "Start: 1118.06s, End: 1118.64s, Text:  What does that mean?\n",
            "Start: 1119.66s, End: 1122.94s, Text:  No, I think, see, I think the interesting thing is,\n",
            "Start: 1122.94s, End: 1125.0s, Text:  and I think that it comes back\n",
            "Start: 1125.0s, End: 1126.7s, Text:  to the fundamental nature of AI.\n",
            "Start: 1127.16s, End: 1129.06s, Text:  AI is a tool, right?\n",
            "Start: 1129.06s, End: 1130.5s, Text:  And you have to use that,\n",
            "Start: 1130.66s, End: 1134.6s, Text:  and you have to use that within your business process, right?\n",
            "Start: 1134.8s, End: 1136.44s, Text:  And how AI is being used,\n",
            "Start: 1136.48s, End: 1138.88s, Text:  and so, and what's going to happen is that,\n",
            "Start: 1138.92s, End: 1141.98s, Text:  I mean, I think this is true with, you know,\n",
            "Start: 1141.98s, End: 1144.18s, Text:  when someone said in terms of, you know, people,\n",
            "Start: 1144.18s, End: 1147.26s, Text:  they said that the people who leverage AI\n",
            "Start: 1147.26s, End: 1151.42s, Text:  will be more effective than those who don't leverage AI.\n",
            "Start: 1151.76s, End: 1154.28s, Text:  And that will stoop for organizations also.\n",
            "Start: 1154.92s, End: 1156.9s, Text:  Organizations that leverage AI\n",
            "Start: 1157.5800000000002s, End: 1159.66s, Text:  fundamentally in their core business processes\n",
            "Start: 1159.66s, End: 1162.78s, Text:  will be more effective than those who don't, right?\n",
            "Start: 1162.78s, End: 1163.92s, Text:  And I think that's the thing,\n",
            "Start: 1163.96s, End: 1166.46s, Text:  and you won't know the difference\n",
            "Start: 1166.46s, End: 1168.9s, Text:  until one day it becomes too obvious,\n",
            "Start: 1169.16s, End: 1170.24s, Text:  and it will be too late.\n",
            "Start: 1170.3s, End: 1172.52s, Text:  And I think that's the reason why everybody\n",
            "Start: 1172.52s, End: 1176.66s, Text:  needs to think about what it means for your business,\n",
            "Start: 1176.98s, End: 1179.26s, Text:  because everything will be fine.\n",
            "Start: 1179.5s, End: 1181.28s, Text:  Everything will be fine, and one day,\n",
            "Start: 1181.8s, End: 1185.58s, Text:  somebody in your, either your competitor in your space\n",
            "Start: 1185.58s, End: 1187.82s, Text:  or somebody brand new coming into your space\n",
            "Start: 1187.82s, End: 1190.84s, Text:  will be reimagining your business process completely.\n",
            "Start: 1191.18s, End: 1193.86s, Text:  And at that stage, you will find that it's, you know,\n",
            "Start: 1193.96s, End: 1198.78s, Text:  it's a very big, very tall, you know, mountain to climb.\n",
            "Start: 1198.8s, End: 1202.16s, Text:  And that's why I think it's important for both people\n",
            "Start: 1202.16s, End: 1206.02s, Text:  and entities to think about how they will, you know,\n",
            "Start: 1206.1s, End: 1207.42s, Text:  they will upgrade themselves,\n",
            "Start: 1207.5s, End: 1210.52s, Text:  or they will modify their business processes to work.\n",
            "Start: 1210.84s, End: 1212.02s, Text:  That's a very nuanced answer,\n",
            "Start: 1212.3s, End: 1213.94s, Text:  and everybody here who's running a business\n",
            "Start: 1213.94s, End: 1215.3s, Text:  should really think about it,\n",
            "Start: 1215.3s, End: 1216.82s, Text:  because life will be the same,\n",
            "Start: 1217.22s, End: 1219.38s, Text:  and then suddenly, suddenly something will, you know,\n",
            "Start: 1219.5s, End: 1220.72s, Text:  then it will be a step change.\n",
            "Start: 1221.96s, End: 1222.8s, Text:  Vivek, I have a few more questions,\n",
            "Start: 1222.82s, End: 1225.36s, Text:  but I'm sure the audience has a lot of questions for you.\n",
            "Start: 1225.66s, End: 1227.2s, Text:  So how are we doing on time?\n",
            "Start: 1228.6s, End: 1231.76s, Text:  Okay, so, does, okay, a lot of questions.\n",
            "Start: 1231.76s, End: 1235.14s, Text:  So love to, is there a mic that we can pass around?\n",
            "Start: 1240.28s, End: 1241.06s, Text:  Thank you.\n",
            "Start: 1241.26s, End: 1242.06s, Text:  My name is Karthik.\n",
            "Start: 1242.18s, End: 1245.94s, Text:  I work for IT service industry.\n",
            "Start: 1246.84s, End: 1252.02s, Text:  So you're saying that you're working on LLM, sorry,\n",
            "Start: 1252.48s, End: 1254.66s, Text:  it's fine-tuned LLM on top of Lama.\n",
            "Start: 1255.04s, End: 1256.98s, Text:  My basic question, fundamental question is,\n",
            "Start: 1257.02s, End: 1259.52s, Text:  we don't have a foundational model for India.\n",
            "Start: 1259.52s, End: 1263.18s, Text:  Most of the models are basically using English,\n",
            "Start: 1263.9s, End: 1264.8s, Text:  or those kind of things.\n",
            "Start: 1264.98s, End: 1269.34s, Text:  For example, Andrew was talking about the tokenizers\n",
            "Start: 1269.34s, End: 1270.08s, Text:  and things like that.\n",
            "Start: 1270.12s, End: 1272.18s, Text:  So are you working on anything like that,\n",
            "Start: 1272.2s, End: 1276.02s, Text:  or do you want to use mostly the existing models\n",
            "Start: 1276.02s, End: 1276.98s, Text:  and run on top of it?\n",
            "Start: 1277.28s, End: 1278.06s, Text:  Are you going to use it?\n",
            "Start: 1278.12s, End: 1280.18s, Text:  Good question, you asked a cherry question for him, sir.\n",
            "Start: 1281.32s, End: 1284.74s, Text:  No, I think the interesting thing is that if you look at,\n",
            "Start: 1284.8s, End: 1287.98s, Text:  and then we have actually a blog on this, on our website,\n",
            "Start: 1287.98s, End: 1289.42s, Text:  I think one of the things that we've built\n",
            "Start: 1289.42s, End: 1292.52s, Text:  is we've actually built a customized tokenizer,\n",
            "Start: 1292.56s, End: 1295.02s, Text:  which actually fundamentally changes the cost\n",
            "Start: 1295.02s, End: 1298.1s, Text:  of some of these generations in Indian languages.\n",
            "Start: 1298.72s, End: 1301.36s, Text:  And I think that we're not just fine-tuning.\n",
            "Start: 1301.56s, End: 1304.82s, Text:  We're actually, we are leveraging the existing pre-training,\n",
            "Start: 1304.88s, End: 1307.4s, Text:  but we are doing what's known as continual free training,\n",
            "Start: 1307.48s, End: 1309.52s, Text:  which actually, but having said that,\n",
            "Start: 1310.14s, End: 1314.2s, Text:  I think that when we have to figure out where is the data\n",
            "Start: 1314.2s, End: 1316.46s, Text:  to train an extremely large model from scratch,\n",
            "Start: 1316.46s, End: 1317.86s, Text:  and some of those things are things\n",
            "Start: 1317.86s, End: 1320.44s, Text:  which will happen over time.\n",
            "Start: 1320.66s, End: 1325.42s, Text:  But I think that, yes, I think that we will be\n",
            "Start: 1325.42s, End: 1326.62s, Text:  doing various kinds of things,\n",
            "Start: 1326.64s, End: 1330.18s, Text:  but the interesting thing is that if I want to change\n",
            "Start: 1330.18s, End: 1332.34s, Text:  the accessibility problem with an existing\n",
            "Start: 1332.34s, End: 1334.36s, Text:  open source model, how do I do that?\n",
            "Start: 1334.36s, End: 1336.08s, Text:  And that's the problem that we have,\n",
            "Start: 1336.34s, End: 1337.72s, Text:  that we think we have solved,\n",
            "Start: 1337.74s, End: 1340.62s, Text:  and is going to be the heart of this open-hearty series.\n",
            "Start: 1340.64s, End: 1342.02s, Text:  It's extremely well explained in the blog,\n",
            "Start: 1342.08s, End: 1343.56s, Text:  even I could understand it, so.\n",
            "Start: 1344.88s, End: 1346.44s, Text:  Hi, I'm Prashant.\n",
            "Start: 1346.48s, End: 1348.08s, Text:  I work for a FinTech company.\n",
            "Start: 1349.14s, End: 1350.7s, Text:  My question is, unlike China,\n",
            "Start: 1350.84s, End: 1353.56s, Text:  we never had a consumer-facing application\n",
            "Start: 1353.56s, End: 1357.76s, Text:  coming out from India, and in WebOne, WebTwo,\n",
            "Start: 1358.0s, End: 1360.78s, Text:  Crypto and all, why do you think it will be different\n",
            "Start: 1360.78s, End: 1364.08s, Text:  this time in AI?\n",
            "Start: 1365.64s, End: 1368.56s, Text:  Because will the DPI and other things\n",
            "Start: 1369.58s, End: 1371.04s, Text:  will serve the same purpose\n",
            "Start: 1371.04s, End: 1372.92s, Text:  what the great fire world did in China?\n",
            "Start: 1372.92s, End: 1377.48s, Text:  Do you think, like, in, because AI is a strategic sector,\n",
            "Start: 1378.4s, End: 1382.72s, Text:  no outside country can work in NASA projects,\n",
            "Start: 1382.96s, End: 1385.44s, Text:  maybe all government contracts will go to them.\n",
            "Start: 1385.72s, End: 1388.62s, Text:  What exactly is the moat here for an Indian company?\n",
            "Start: 1391.12s, End: 1395.78s, Text:  So, I don't, I think the question is,\n",
            "Start: 1396.34s, End: 1398.62s, Text:  I don't know the answer to these questions, right?\n",
            "Start: 1398.62s, End: 1402.2s, Text:  I mean, I think that it's difficult to predict,\n",
            "Start: 1402.2s, End: 1404.82s, Text:  but I do believe, and as I'm repeating,\n",
            "Start: 1405.4s, End: 1408.86s, Text:  that the combinatorial effect of being using GenAI\n",
            "Start: 1408.86s, End: 1410.66s, Text:  at a large scale in addition,\n",
            "Start: 1410.96s, End: 1413.54s, Text:  along with the DPI work that we've done in India,\n",
            "Start: 1413.98s, End: 1416.34s, Text:  will have people, and I think that in the end,\n",
            "Start: 1417.04s, End: 1421.36s, Text:  it is, the intent is that people need to be able to use it,\n",
            "Start: 1421.36s, End: 1423.72s, Text:  and they will vote by things that are useful for them,\n",
            "Start: 1423.72s, End: 1426.32s, Text:  and if that doesn't happen, you're right,\n",
            "Start: 1426.52s, End: 1429.9s, Text:  that, and I think that we have to figure out\n",
            "Start: 1429.9s, End: 1432.64s, Text:  what is the mechanism of delivery of apps, right?\n",
            "Start: 1432.68s, End: 1436.36s, Text:  I mean, where do Indians consume content?\n",
            "Start: 1436.44s, End: 1436.92s, Text:  That's the question.\n",
            "Start: 1437.42s, End: 1439.2s, Text:  I'm so sorry, but we are out of time.\n",
            "Start: 1439.42s, End: 1442.64s, Text:  Vivek will be outside, so he would be able\n",
            "Start: 1442.64s, End: 1443.18s, Text:  to answer the question.\n",
            "Start: 1443.26s, End: 1444.06s, Text:  Do we have time for one last question?\n",
            "Start: 1444.06s, End: 1446.18s, Text:  Can I just take one last?\n",
            "Start: 1446.7s, End: 1447.96s, Text:  Yeah, thank you, thank you.\n",
            "Start: 1448.14s, End: 1450.62s, Text:  I'm Manish Kothari, I'm from ISBR Business School.\n",
            "Start: 1450.72s, End: 1452.9s, Text:  Good that I got a chance to ask you this question.\n",
            "Start: 1453.44s, End: 1456.12s, Text:  During lunchtime, there were a few of our educationists\n",
            "Start: 1456.12s, End: 1459.28s, Text:  whom we were talking about, and there was one from school,\n",
            "Start: 1459.28s, End: 1461.42s, Text:  and we are from the MBA institutions.\n",
            "Start: 1461.62s, End: 1463.84s, Text:  We were thinking of these present generations,\n",
            "Start: 1463.92s, End: 1466.62s, Text:  how do we get them into what you are doing?\n",
            "Start: 1467.6s, End: 1469.46s, Text:  There is one thing that they have been regularly,\n",
            "Start: 1469.64s, End: 1471.5s, Text:  that the concentrations that they're working on,\n",
            "Start: 1471.66s, End: 1473.96s, Text:  but artificial intelligence and getting into this,\n",
            "Start: 1474.32s, End: 1475.66s, Text:  getting them into their academics\n",
            "Start: 1475.66s, End: 1478.14s, Text:  and making them a part of it is very important,\n",
            "Start: 1478.32s, End: 1479.98s, Text:  including the trainers who train them,\n",
            "Start: 1480.46s, End: 1483.68s, Text:  making them future ready into what you are doing is amazing,\n",
            "Start: 1483.68s, End: 1486.14s, Text:  and the speed that which is growing,\n",
            "Start: 1486.14s, End: 1489.68s, Text:  it is calling for a lot of training that needs to be done.\n",
            "Start: 1490.26s, End: 1492.84s, Text:  Can you, from your angle, throw some light on\n",
            "Start: 1492.84s, End: 1494.62s, Text:  how we could make them future ready?\n",
            "Start: 1495.0s, End: 1498.24s, Text:  How these people who are management graduates\n",
            "Start: 1498.24s, End: 1500.36s, Text:  and from schools who are coming out,\n",
            "Start: 1500.62s, End: 1502.64s, Text:  how do we get into this part of technology\n",
            "Start: 1502.64s, End: 1503.56s, Text:  that you spoke about?\n",
            "Start: 1504.86s, End: 1507.14s, Text:  No, so this is really a challenge,\n",
            "Start: 1507.24s, End: 1510.36s, Text:  because I think everyone will need to understand\n",
            "Start: 1510.36s, End: 1513.02s, Text:  at some level what this technology does,\n",
            "Start: 1513.02s, End: 1515.1s, Text:  and I think that we have to rethink\n",
            "Start: 1515.1s, End: 1518.14s, Text:  how we get everyone into this,\n",
            "Start: 1518.22s, End: 1520.32s, Text:  and this kind of education has to be\n",
            "Start: 1520.32s, End: 1522.14s, Text:  at many different levels, right?\n",
            "Start: 1522.14s, End: 1525.04s, Text:  There are, from a core set of having people\n",
            "Start: 1525.04s, End: 1528.34s, Text:  who are extremely good at some,\n",
            "Start: 1528.5s, End: 1529.84s, Text:  and there you don't need as many,\n",
            "Start: 1530.1s, End: 1532.56s, Text:  but then there are basically vast numbers of people\n",
            "Start: 1532.56s, End: 1534.04s, Text:  who can actually leverage these tools.\n",
            "Start: 1534.26s, End: 1536.64s, Text:  By the way, the most important thing about,\n",
            "Start: 1536.82s, End: 1539.22s, Text:  and maybe that's part of what makes an LLM interesting,\n",
            "Start: 1539.68s, End: 1544.38s, Text:  is that how you use it, your mileage varies by that,\n",
            "Start: 1544.38s, End: 1547.3s, Text:  and to understand how to actually leverage this\n",
            "Start: 1547.3s, End: 1548.94s, Text:  in an interesting way is something\n",
            "Start: 1548.94s, End: 1552.38s, Text:  that we have to widely teach many, many people,\n",
            "Start: 1553.42s, End: 1557.56s, Text:  and because asking the things in the right way\n",
            "Start: 1557.56s, End: 1559.24s, Text:  and having the right kind of applications\n",
            "Start: 1559.24s, End: 1561.62s, Text:  will make a huge difference to how people\n",
            "Start: 1561.62s, End: 1562.9s, Text:  can leverage these tools.\n",
            "Start: 1563.26s, End: 1564.1s, Text:  Awesome. Thank you.\n",
            "Start: 1564.14s, End: 1565.42s, Text:  Thank you very much, Vivek.\n",
            "Start: 1565.7s, End: 1567.58s, Text:  Very good luck to Sarvam and good luck to India.\n",
            "Start: 1567.74s, End: 1569.94s, Text:  I think it's going to be a lot right on your shoulders.\n",
            "Start: 1570.66s, End: 1571.54s, Text:  Thanks, Bala.\n",
            "Start: 1573.5s, End: 1573.82s, Text:  Thank you.\n",
            "Start: 1573.82s, End: 1574.72s, Text:  Thank you, Mr. Raghavan.\n"
          ]
        }
      ],
      "source": [
        "# Ensure that a GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the medium Whisper model with FP16 precision\n",
        "model = whisper.load_model(\"medium\", device=device)\n",
        "\n",
        "# Step 3: Transcribe the audio file\n",
        "result = model.transcribe(\"audio.wav\", fp16=True,word_timestamps = True)\n",
        "\n",
        "# Print the transcription\n",
        "segments = result[\"segments\"]\n",
        "for segment in segments:\n",
        "    print(f\"Start: {segment['start']}s, End: {segment['end']}s, Text: {segment['text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhCmD3RdgK_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b91cf99-40b9-4edd-a40d-f82118240733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 0:  Congratulations to you Mr. Raghavan for that.  Thank you so much for joining us.  Over to you.  Hi everybody.  How are you?  Okay, I am not hearing this at all.  It's like a post lunch energy downer or something.  Let's hear it.  Are you guys awake?  All right. (Start: 0.00s)\n",
            "Chunk 1:  You better be because we have a superstar guest here.  You heard the $41 million  and I didn't hear honestly anything she said after that.  So we're gonna ask for about $40 million from him  by the end of this conversation, okay?  But let's get started. (Start: 23.02s)\n",
            "Chunk 2:  I wanna introduce Vivek and Pratyush,  his co-founder who's not here.  We wanted to start with playing a video  of what OpenHati does.  I encourage all of you to go to the website,  sauram.ai and check it out.  But let me start by introducing Vivek. (Start: 41.88s)\n",
            "Chunk 3:  Vivek is a dear friend and he's very, very modest.  One of the most modest guys that I know.  But his personal journey, Vivek,  you've got a PhD from Carnegie Mellon,  you started and sold the company to Magma.  And Vivek and I moved back to India (Start: 58.58s)\n",
            "Chunk 4:  from we were both in the valley on the same day actually.  And you've been in India for the last 16 years.  And what most people don't know is your journey at Aadhaar.  He spent 13 years selflessly at Aadhaar.  Nobody would have heard of him, (Start: 74.00s)\n",
            "Chunk 5:  but he was a pioneering technology visionary behind Aadhaar  which we all take for granted today.  So please give it out.  So honestly, when people,  when I think of selfless service,  truly selfless service, I always think of Vivek. (Start: 90.66s)\n",
            "Chunk 6:  And since then, he also was at AI for Bharat,  which we're gonna touch on,  where he met Pratyush's other co-founder.  Pratyush had a PhD from ETH at Zurich.  He was at IBM research.  He was at Microsoft research playing a key role (Start: 107.92s)\n",
            "Chunk 7:  and a faculty at IIT Madras and at AI for Bharat.  So that's a little brief introduction about them.  These guys are modest, modest engineers.  So they don't toot their own horn.  So forgive me for tooting their horn in this case.  But let's jump right in. (Start: 122.66s)\n",
            "Chunk 8:  About the money, funding, 41 million bucks, man.  That's a lot of money, right?  Every entrepreneur here is saying,  what the hell did these guys do?  What did the investors see to write such a big check?  No, I think it's a trend, (Start: 140.60s)\n",
            "Chunk 9:  the new trend of what's going on in India.  I think that for the very first time,  I think the investors have looked at,  let's try and build something deep tech out of the country  and let's try to figure out how to build something  as a foundational technology out of the country. (Start: 155.90s)\n",
            "Chunk 10:  And that's really what's really exciting.  And I think that about,  as Bala was mentioning for the past 15 years,  I've been kind of working in kind of,  both digital public infrastructure (Start: 170.92s)\n",
            "Chunk 11:  and kind of a nonprofit kind of things.  But when this whole thing of generative AI came about,  we said, okay, how can I actually make a difference  in this space?  And I said, maybe this is the opportunity (Start: 187.58s)\n",
            "Chunk 12:  to actually come out and really build something.  And the only way that we realized that you can do it  is actually in the private sector.  And I think that's, and then we went out there  and we said, we want to build something (Start: 201.52s)\n",
            "Chunk 13:  which is a continuation, right?  I mean, and fundamentally the question is,  the reason of what we want to do at ServoAI  is we want to basically make generative AI available  and accessible to the people in the country.  And that's the intent. (Start: 216.58s)\n",
            "Chunk 14:  And when we said that we want to do this,  there was a resonance in the investment community.  And I think it's a responsibility to really to show  that something like this can be built out of India.  So we see that as confidence and a responsibility. (Start: 231.62s)\n",
            "Chunk 15:  And I also hope it's a trend  that there are many more people like us who are backed.  Because if you look at it,  maybe it's a large number in the Indian context,  but in the global context,  I think there should be many, many more entrepreneurs (Start: 246.92s)\n",
            "Chunk 16:  who are back to do things in India.  Yeah, I'm going to come back to the many more entrepreneurs.  I'm obviously going to ask you about Bhavesh's Krutrim.  So we want to come back to that question.  But again, $41 million.  I mean, all of what you said, you know, $2 million, (Start: 262.52s)\n",
            "Chunk 17:  that's a good amount of money for a startup  which has not yet built anything.  What are you going to do with all this money?  I can solve the problem.  I can have a perfect solution for the problem.  I think in the last week,  I've got lots of calls from lots of people  telling me how I can do.  No, but... (Start: 279.70s)\n",
            "Chunk 18:  I know you first, okay?  I'll be landed in the country the same day.  I'm in the front of the queue.  No, but honestly, I think the key thing in this  is to putting together an amazing team.  And we actually have an amazing team,  but we believe that it is talent  that will drive this kind of thing. (Start: 298.46s)\n",
            "Chunk 19:  And so it is to get key talent.  And of course, the other thing is compute.  This is extremely expensive, compute-wise,  to actually do these kinds of things.  And I think that those are the two primary things  that we'd use this for.  Okay. (Start: 315.22s)\n",
            "Chunk 20:  I'm computing in my own head as an entrepreneur.  Talent, okay, you have like 20, 15 people.  How much are you paying these guys?  But okay, we won't touch on that.  But let's talk about what you guys actually built.  What is OpenHati?  How would you explain OpenHati to many people here  who might not have known about it? (Start: 332.84s)\n",
            "Chunk 21:  So I think OpenHati is, so first of all, right,  we come from, I personally come from the open source  ecosystem and also from the DPI ecosystem.  So we believe that for this to work,  we need the ecosystem to be successful. (Start: 349.54s)\n",
            "Chunk 22:  And as a result of that, one of the first things we did was,  hey, there are these open source  large language models that exist, right?  I mean, everybody knows about the llama family from Meta.  There are others like Mistral. (Start: 365.26s)\n",
            "Chunk 23:  There are a bunch of open source large language models.  And then we said, is there any way  that they can existing open source model  and teach it language skills, right? (Start: 378.66s)\n",
            "Chunk 24:  I mean, and that is really what we said  that can we do something like that?  And is this a relatively frugal way (Start: 390.88s)\n",
            "Chunk 25:  of actually making models work in a way  in diverse languages?  Because the truth is still today,  I mean, if you look at the amount of data and knowledge,  it is still English dominates these things. (Start: 400.22s)\n",
            "Chunk 26:  And I think that how do you actually take  and make it understand Indian language,  understand Indian context, and all of those things  in actually in an efficient way?  And therefore, this was an attempt to do that. (Start: 415.30s)\n",
            "Chunk 27:  And it's a OpenHati is currently based on the llama  seven billion model, but we'll be releasing  many more models in different languages,  different sizes, and things like that  as part of this series. (Start: 428.08s)\n",
            "Chunk 28:  And of course, we will be building further models  on those and doing other things to actually,  and we'll also have endpoints that people can use.  So therefore, it's definitely something  that people can use to things. (Start: 442.56s)\n",
            "Chunk 29:  And that's the essence of what this OpenHati is.  So what does it mean to people in the audience here  who are either doing their own startups  or a business or developers,  how should they look at OpenAI?  Sorry, Sarvam. (Start: 456.14s)\n",
            "Chunk 30:  No, no, I think the way you look at it is that  one of the important things that we are doing  is we're not just building models.  We are also going to be building a platform, (Start: 470.98s)\n",
            "Chunk 31:  a platform for developers where you can actually use  a combination of various different kinds of models,  some which are from us, some which are open source,  some which may not be open source,  and actually to actually pull together and figure out (Start: 484.24s)\n",
            "Chunk 32:  how to deploy generative AI applications at scale  and understand and evaluate their performance  in an efficient manner.  And that's something that we are planning to do.  And this platform is, in the next couple of months, (Start: 497.26s)\n",
            "Chunk 33:  will be coming out there.  It will be available to developers.  But of course, those who want to start  with the open source things and hack with that,  of course, please go ahead and do that as well.  That's phenomenal.  But how does it compare to OpenAI itself or Google? (Start: 512.82s)\n",
            "Chunk 34:  See, at least the things that we are doing now.  One of the things that, when we thought about  building Sarvam, we said we want to build  a full-stack generative AI company,  and different people have, and our understanding  of full-stack is that we need to know (Start: 530.30s)\n",
            "Chunk 35:  how to train models from scratch.  We need to know how to figure out how to deploy models  to solve real-world use cases.  And we need to play in the ecosystem to make sure (Start: 545.94s)\n",
            "Chunk 36:  that we can actually deploy population scale applications.  So we were thinking about all of these things.  But still, the models we were talking about  are fairly small models.  They are fairly small models, right? (Start: 557.26s)\n",
            "Chunk 37:  The seven to maybe up to 70 billion kind of range  we're talking about.  While these models like OpenAI and Google  are obviously much bigger models, right?  But we want to understand the techniques  and be able to build that muscle (Start: 570.86s)\n",
            "Chunk 38:  to do all of these things, to make it available to people.  Now those models are, I mean, as I said,  I think that there is space for all of those things.  And I think as even Sridhar was talking about earlier (Start: 585.32s)\n",
            "Chunk 39:  in the day, we believe that these smaller models  can do many, many kind of domain specific tasks  extremely well, probably even better than the larger models.  And that is really one of the key areas. (Start: 600.10s)\n",
            "Chunk 40:  And so therefore the value of these kinds of things, right?  We are not aiming in these more set of models  to build any AGI, right?  That's not our goal here.  Our goal is to make things that work extremely well  for domain specific use cases (Start: 615.14s)\n",
            "Chunk 41:  or increase accessibility through language  and all of those kinds of things.  And obviously all of this unique to India.  But what is unique about India?  I mean, like what is, is there anything special  in our ecosystem that makes small models (Start: 628.80s)\n",
            "Chunk 42:  focused with Indian languages better for,  more suited for our problems?  So I think that, I mean, there are quite a few things  that are unique about India, right?  The first thing is I think that we are a voice first nation. (Start: 643.30s)\n",
            "Chunk 43:  So therefore I think voice has to be the core  to doing things.  The other thing, of course, India is extremely,  it's a cost conscious country from a cost perspective.  Now, I would say that there are lots of interesting (Start: 657.72s)\n",
            "Chunk 44:  use cases where you can use open AI  and the cost structure works that when,  depending on your application.  But when you want to scale things to a massive level  and make it work, then you have to figure out  how small models work. (Start: 672.50s)\n",
            "Chunk 45:  So that's something that is also specific to India.  The third thing which is specific to India  is really the success that India has had  in building all this digital public infrastructure.  When you add the AI layer on top of it, (Start: 685.12s)\n",
            "Chunk 46:  then you can actually get dramatic, you know,  dramatic, I think, multiplicative combinatorial effects  based on doing things like that.  That's a phenomenal point.  Like, you know, it's like DPI to the power of AI  almost in some ways.  And as a part of building other, (Start: 698.74s)\n",
            "Chunk 47:  no better person than you.  So in summary, what I'm hearing is small models  specialize with, trained with Indic specific language data  suited for Indian problems at a compelling cost point  will be suited for us. (Start: 714.34s)\n",
            "Chunk 48:  We're not solving some word autonomous vehicles  or some complex problem.  We're solving some basic problems  specifically focused on voice with multiple languages.  That is what you see as the future.  Am I paraphrasing this correctly?  No, yeah.  So I think that certainly, I mean, (Start: 728.94s)\n",
            "Chunk 49:  voice and Indian languages are an important part  of our strategy, but we will be building, you know,  custom models to solve various other kinds of problems  as well, right?  It's not just limited to, I think, in different domains,  working in different domains, (Start: 744.06s)\n",
            "Chunk 50:  making building things based on unique data  that enterprises have and things like that.  So that's something that we'll also look at.  Fair enough.  So coming back to the elephant in the room,  no pun intended with open Hathi.  What about Babi Shakerwal and Kruthram?  What is your take on that? (Start: 758.92s)\n",
            "Chunk 51:  No, I think it's great.  I think it's wonderful, right?  I mean, the fact that the technology AI is so important  that we need multiple people working on it.  The fact that there are other people thinking (Start: 775.38s)\n",
            "Chunk 52:  is actually validates that this is an important problem  to be solved.  And I think that, and we need everybody to come together  and do that.  So I really welcome that.  I think it's great.  And I think that there'll be different people (Start: 787.76s)\n",
            "Chunk 53:  will have different takes  as to how to solve this kind of problem.  And hopefully as a result of that,  the entire ecosystem benefits.  I have one more question.  And then I wanna talk about some of the predictions  that you've boldly made.  So Vivek, I usually ask people about,  what do you think the future will be? (Start: 802.88s)\n",
            "Chunk 54:  And everybody usually hedges.  I asked Vivek, what do you think is gonna happen  by December, 2024?  What do you think sitting in this room,  one year later, we can expect?  And he made three bold predictions.  So I wanna talk about that.  Before that, I have one last question.  What are the top three applications  that you think are relevant for India? (Start: 819.28s)\n",
            "Chunk 55:  You heard Sridhar talk about medical.  What, no, any quick summary.  What do you think the top three apps are for India, for AI?  So I mean, I think that, as you said,  things like education and medical are clearly areas  where I think that things can be leveraged. (Start: 838.34s)\n",
            "Chunk 56:  The whole idea of all these kind of,  the DPI aspect of it is another major application  where things can happen.  And here I'm talking about country-specific work.  And I think the whole idea, which Sridhar also talked about,  was the concept of software, right? (Start: 855.04s)\n",
            "Chunk 57:  And I think that, and clearly we have  a very large software industry,  and how to reimagine those things in this context  is also something that's gonna be.  Fair enough.  Are you guys ready for Vivek Raghavan's bold predictions?  Yes?  No, I'm not hearing any yeses.  This is like a big deal. (Start: 870.00s)\n",
            "Chunk 58:  He's like one of the smartest guys that I know.  He wants to make three predictions.  You don't want to hear it?  All right.  So I asked him, what do you think, a year later,  what do you think we can expect?  And he came up with three things,  and usually people give very blah answers  when you ask a question like this, (Start: 888.06s)\n",
            "Chunk 59:  because they don't want to be caught wrong.  Not Vivek, Vivek is bold.  So he basically said three things,  and I'm gonna list out the three things  and then ask him about it.  So number one, he says,  I would prefer to talk to an automated customer service  than a real person,  because they'll give me a better answer. (Start: 904.46s)\n",
            "Chunk 60:  So that is Vivek Raghavan's prediction number one.  So number two is that when everybody is talking  about a GPU shortage,  Vivek predicts that there'll be a GPU glut in India.  He thinks there'll be too much GPU.  So if you want a short NVIDIA stock, this is a good time.  And number three, which was extremely unexpected, (Start: 921.60s)\n",
            "Chunk 61:  he said some companies will suddenly die.  Okay, so Vivek, these are not what I expected.  So do you want to quickly talk about each of them,  why you just came up with these,  and then we'll throw the open for audience questions.  So I don't think I quite said it the way that  Pallavi is panicking.  Sorry, I'm the marketing guy here. (Start: 941.98s)\n",
            "Chunk 62:  But it's interesting.  But I think that the first thing that we said is,  I think that, and I don't think that this is,  I think there will come a time when,  in areas of customer service, et cetera, (Start: 962.44s)\n",
            "Chunk 63:  when you want to do something very specific.  Today, when you call some kind of a bot,  you actually end up, you mostly try to disconnect the call,  or you're extremely upset that you're talking to a bot.  But I think that there will come a time, (Start: 977.66s)\n",
            "Chunk 64:  and I'm predicting it is sooner than later,  that you will actually get better responses from the bot  than what the human representative,  at least the average human representative  that you could talk to, could give.  And I think that that's just, (Start: 993.06s)\n",
            "Chunk 65:  I just said that there will come a time where you know  it's not a human you're talking to,  but it's probably more likely to solve your intent  than the human person.  That's just something that I think could happen. (Start: 1007.66s)\n",
            "Chunk 66:  Okay, definitely controversial, but we'll let it go.  What about the GPU glut?  No, no, yeah, so I don't think that,  so I think that the fact that there is  a tremendous shortage right now,  I think that shortage will ease, (Start: 1023.36s)\n",
            "Chunk 67:  because that is how the cycles of things go, right?  When, you know, I think the fact that  there was such a severe shortage last year,  you know, basically caused a number of different players  to ramp up in various kinds of forms. (Start: 1036.90s)\n",
            "Chunk 68:  And I think that that will always go in a cycle.  But you may, we may find out that there are many,  many more interesting problems  that people will be able to solve.  I still remember, you know, (Start: 1051.32s)\n",
            "Chunk 69:  we were at a Gen. AI event in Bangalore,  and we were talking to people, and we said,  you know, how many people have access to,  you know, four A-hundreds?  This was the question that I'd asked,  and nobody in the room,  and these are all extremely enthusiastic Gen. AI people, (Start: 1065.00s)\n",
            "Chunk 70:  and nobody had access.  And I think that thing is going to change.  You will be able to get these kinds of things,  and people who want to hack and do things  will have access to these things  at, without, you know, having to write a, you know,  a major check. (Start: 1080.54s)\n",
            "Chunk 71:  So Vivek is also a semiconductor guy  before he went into Aadhaar,  so I would take his predictions very seriously.  So I don't know what I, I'm going to sell my in-media stock.  No, no, no, I would not do that,  but that's not what I said.  I want to blame you for this, if it goes up.  But the third one is pretty strange. (Start: 1095.02s)\n",
            "Chunk 72:  You know, companies are born, companies die,  but you said some companies will suddenly die.  What does that mean?  No, I think, see, I think the interesting thing is,  and I think that it comes back  to the fundamental nature of AI.  AI is a tool, right? (Start: 1113.08s)\n",
            "Chunk 73:  And you have to use that,  and you have to use that within your business process, right?  And how AI is being used,  and so, and what's going to happen is that,  I mean, I think this is true with, you know,  when someone said in terms of, you know, people, (Start: 1129.06s)\n",
            "Chunk 74:  they said that the people who leverage AI  will be more effective than those who don't leverage AI.  And that will stoop for organizations also.  Organizations that leverage AI  fundamentally in their core business processes (Start: 1144.18s)\n",
            "Chunk 75:  will be more effective than those who don't, right?  And I think that's the thing,  and you won't know the difference  until one day it becomes too obvious,  and it will be too late.  And I think that's the reason why everybody (Start: 1159.66s)\n",
            "Chunk 76:  needs to think about what it means for your business,  because everything will be fine.  Everything will be fine, and one day,  somebody in your, either your competitor in your space  or somebody brand new coming into your space (Start: 1172.52s)\n",
            "Chunk 77:  will be reimagining your business process completely.  And at that stage, you will find that it's, you know,  it's a very big, very tall, you know, mountain to climb.  And that's why I think it's important for both people (Start: 1187.82s)\n",
            "Chunk 78:  and entities to think about how they will, you know,  they will upgrade themselves,  or they will modify their business processes to work.  That's a very nuanced answer,  and everybody here who's running a business  should really think about it,  because life will be the same, (Start: 1202.16s)\n",
            "Chunk 79:  and then suddenly, suddenly something will, you know,  then it will be a step change.  Vivek, I have a few more questions,  but I'm sure the audience has a lot of questions for you.  So how are we doing on time?  Okay, so, does, okay, a lot of questions.  So love to, is there a mic that we can pass around? (Start: 1217.22s)\n",
            "Chunk 80:  Thank you.  My name is Karthik.  I work for IT service industry.  So you're saying that you're working on LLM, sorry,  it's fine-tuned LLM on top of Lama.  My basic question, fundamental question is, (Start: 1240.28s)\n",
            "Chunk 81:  we don't have a foundational model for India.  Most of the models are basically using English,  or those kind of things.  For example, Andrew was talking about the tokenizers  and things like that.  So are you working on anything like that, (Start: 1257.02s)\n",
            "Chunk 82:  or do you want to use mostly the existing models  and run on top of it?  Are you going to use it?  Good question, you asked a cherry question for him, sir.  No, I think the interesting thing is that if you look at,  and then we have actually a blog on this, on our website, (Start: 1272.20s)\n",
            "Chunk 83:  I think one of the things that we've built  is we've actually built a customized tokenizer,  which actually fundamentally changes the cost  of some of these generations in Indian languages.  And I think that we're not just fine-tuning. (Start: 1287.98s)\n",
            "Chunk 84:  We're actually, we are leveraging the existing pre-training,  but we are doing what's known as continual free training,  which actually, but having said that,  I think that when we have to figure out where is the data  to train an extremely large model from scratch, (Start: 1301.56s)\n",
            "Chunk 85:  and some of those things are things  which will happen over time.  But I think that, yes, I think that we will be  doing various kinds of things,  but the interesting thing is that if I want to change (Start: 1316.46s)\n",
            "Chunk 86:  the accessibility problem with an existing  open source model, how do I do that?  And that's the problem that we have,  that we think we have solved,  and is going to be the heart of this open-hearty series.  It's extremely well explained in the blog,  even I could understand it, so.  Hi, I'm Prashant. (Start: 1330.18s)\n",
            "Chunk 87:  I work for a FinTech company.  My question is, unlike China,  we never had a consumer-facing application  coming out from India, and in WebOne, WebTwo,  Crypto and all, why do you think it will be different (Start: 1346.48s)\n",
            "Chunk 88:  this time in AI?  Because will the DPI and other things  will serve the same purpose  what the great fire world did in China?  Do you think, like, in, because AI is a strategic sector, (Start: 1360.78s)\n",
            "Chunk 89:  no outside country can work in NASA projects,  maybe all government contracts will go to them.  What exactly is the moat here for an Indian company?  So, I don't, I think the question is, (Start: 1378.40s)\n",
            "Chunk 90:  I don't know the answer to these questions, right?  I mean, I think that it's difficult to predict,  but I do believe, and as I'm repeating,  that the combinatorial effect of being using GenAI  at a large scale in addition, (Start: 1396.34s)\n",
            "Chunk 91:  along with the DPI work that we've done in India,  will have people, and I think that in the end,  it is, the intent is that people need to be able to use it,  and they will vote by things that are useful for them,  and if that doesn't happen, you're right, (Start: 1410.96s)\n",
            "Chunk 92:  that, and I think that we have to figure out  what is the mechanism of delivery of apps, right?  I mean, where do Indians consume content?  That's the question.  I'm so sorry, but we are out of time. (Start: 1426.52s)\n",
            "Chunk 93:  Vivek will be outside, so he would be able  to answer the question.  Do we have time for one last question?  Can I just take one last?  Yeah, thank you, thank you.  I'm Manish Kothari, I'm from ISBR Business School.  Good that I got a chance to ask you this question. (Start: 1439.42s)\n",
            "Chunk 94:  During lunchtime, there were a few of our educationists  whom we were talking about, and there was one from school,  and we are from the MBA institutions.  We were thinking of these present generations,  how do we get them into what you are doing?  There is one thing that they have been regularly, (Start: 1453.44s)\n",
            "Chunk 95:  that the concentrations that they're working on,  but artificial intelligence and getting into this,  getting them into their academics  and making them a part of it is very important,  including the trainers who train them,  making them future ready into what you are doing is amazing, (Start: 1469.64s)\n",
            "Chunk 96:  and the speed that which is growing,  it is calling for a lot of training that needs to be done.  Can you, from your angle, throw some light on  how we could make them future ready?  How these people who are management graduates (Start: 1483.68s)\n",
            "Chunk 97:  and from schools who are coming out,  how do we get into this part of technology  that you spoke about?  No, so this is really a challenge,  because I think everyone will need to understand  at some level what this technology does, (Start: 1498.24s)\n",
            "Chunk 98:  and I think that we have to rethink  how we get everyone into this,  and this kind of education has to be  at many different levels, right?  There are, from a core set of having people (Start: 1513.02s)\n",
            "Chunk 99:  who are extremely good at some,  and there you don't need as many,  but then there are basically vast numbers of people  who can actually leverage these tools.  By the way, the most important thing about,  and maybe that's part of what makes an LLM interesting, (Start: 1525.04s)\n",
            "Chunk 100:  is that how you use it, your mileage varies by that,  and to understand how to actually leverage this  in an interesting way is something  that we have to widely teach many, many people, (Start: 1539.68s)\n",
            "Chunk 101:  and because asking the things in the right way  and having the right kind of applications  will make a huge difference to how people  can leverage these tools.  Awesome. Thank you.  Thank you very much, Vivek.  Very good luck to Sarvam and good luck to India. (Start: 1553.42s)\n",
            "Chunk 102:  I think it's going to be a lot right on your shoulders.  Thanks, Bala.  Thank you.  Thank you, Mr. Raghavan. (Start: 1567.74s)\n"
          ]
        }
      ],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Load the original audio\n",
        "audio = AudioSegment.from_wav(\"audio.wav\")\n",
        "\n",
        "# Get the segments with timestamps\n",
        "segments = result[\"segments\"]\n",
        "\n",
        "# Initialize variables\n",
        "max_duration = 15 * 1000  # 15 seconds in milliseconds\n",
        "audio_text_pairs = []\n",
        "current_audio = AudioSegment.empty()\n",
        "current_text = \"\"\n",
        "current_start = 0\n",
        "\n",
        "# Process each segment\n",
        "for segment in segments:\n",
        "    start_time = segment['start'] * 1000  # Convert to milliseconds\n",
        "    end_time = segment['end'] * 1000  # Convert to milliseconds\n",
        "    text = segment['text']\n",
        "\n",
        "    # Check if adding this segment exceeds the max duration\n",
        "    if len(current_audio) + (end_time - start_time) > max_duration:\n",
        "        # Save the current pair and reset\n",
        "        audio_text_pairs.append((current_audio, current_text, current_start))\n",
        "        current_audio = AudioSegment.empty()\n",
        "        current_text = \"\"\n",
        "        current_start = start_time\n",
        "\n",
        "    # Add the current segment to the audio and text\n",
        "    current_audio += audio[start_time:end_time]\n",
        "    current_text += \" \" + text if current_text else text\n",
        "\n",
        "# Add the last segment if it exists\n",
        "if current_audio:\n",
        "    audio_text_pairs.append((current_audio, current_text, current_start))\n",
        "\n",
        "# Save the audio segments and display text\n",
        "for i, (audio_segment, text, start_time) in enumerate(audio_text_pairs):\n",
        "    # Save the audio segment\n",
        "    file_name = f\"chunk_{i}.wav\"\n",
        "    audio_segment.export(file_name, format=\"wav\")\n",
        "    print(f\"Chunk {i}: {text} (Start: {start_time / 1000:.2f}s)\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMILYoKiUjtVVhzEj8p+em+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}